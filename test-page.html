<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=        <div class=" controls">
  <button onclick="startWebcamVideo()" id="webcamBtn">ğŸ“· Start Webcam Test</button>
  <button onclick="startScreenShare()" id="screenBtn">ğŸ–¥ï¸ Start Screen Share Test</button>
  <button onclick="createSpeechTest()" id="speechBtn">ğŸ—£ï¸ Start Speech Test</button>
  <button onclick="startSynthesizedSpeech()" id="synthBtn">ğŸ”Š Generate Test Speech</button>
  <button onclick="stopAllStreams()" id="stopBtn">â¹ï¸ Stop All</button>
  </div>
  <title>Meeting Video to Text - Test Page</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      max-width: 1200px;
      margin: 0 auto;
      padding: 20px;
      background-color: #f5f5f5;
    }

    .container {
      background: white;
      padding: 30px;
      border-radius: 10px;
      box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
    }

    .video-container {
      margin: 20px 0;
      padding: 20px;
      border: 2px solid #ddd;
      border-radius: 8px;
      background: #f9f9f9;
    }

    video {
      width: 100%;
      max-width: 640px;
      height: auto;
      border-radius: 5px;
    }

    .controls {
      margin: 20px 0;
      text-align: center;
    }

    button {
      background: #4285f4;
      color: white;
      border: none;
      padding: 12px 24px;
      border-radius: 5px;
      cursor: pointer;
      margin: 0 10px;
      font-size: 16px;
    }

    button:hover {
      background: #3367d6;
    }

    button:disabled {
      background: #ccc;
      cursor: not-allowed;
    }

    .instructions {
      background: #e3f2fd;
      padding: 20px;
      border-radius: 5px;
      margin: 20px 0;
    }

    .console-output {
      background: #1e1e1e;
      color: #00ff00;
      padding: 15px;
      border-radius: 5px;
      font-family: 'Courier New', monospace;
      height: 200px;
      overflow-y: auto;
      margin: 20px 0;
    }

    .platform-selector {
      margin: 20px 0;
      text-align: center;
    }

    select {
      padding: 8px 12px;
      border-radius: 5px;
      border: 1px solid #ddd;
      font-size: 16px;
    }

    .status {
      background: #e8f5e8;
      padding: 15px;
      border-radius: 5px;
      margin: 20px 0;
      border-left: 4px solid #4caf50;
    }
  </style>
</head>

<body>
  <div class="container">
    <h1>ğŸ¥ Meeting Video to Text Extension - Test Page</h1>

    <div class="status">
      <h3>ğŸš€ Extension Status</h3>
      <p>Open Chrome Developer Console (F12) to see detailed logs from the extension.</p>
      <p>The extension should automatically detect and process any video elements on this page.</p>
    </div>

    <div class="instructions">
      <h3>ğŸ“‹ Testing Instructions</h3>
      <ol>
        <li><strong>Open Chrome DevTools</strong> (F12) and go to the Console tab</li>
        <li><strong>Grant microphone permissions</strong> when prompted</li>
        <li><strong>Start a test video</strong> using the buttons below</li>
        <li><strong>Speak into your microphone</strong> - you should see transcription logs in the console</li>
        <li><strong>Look for log messages</strong> like:
          <ul>
            <li>"Video detected on platform: generic"</li>
            <li>"Audio extraction started"</li>
            <li>"ğŸ“ Final Transcript: [your speech]"</li>
          </ul>
        </li>
      </ol>
    </div>

    <div class="platform-selector">
      <label for="platform">Test Platform Mode:</label>
      <select id="platform" onchange="updatePageUrl()">
        <option value="generic">Generic Platform</option>
        <option value="meet.google.com">Google Meet</option>
        <option value="zoom.us">Zoom</option>
        <option value="teams.microsoft.com">Microsoft Teams</option>
      </select>
    </div>

    <div class="video-container">
      <h3>ğŸ“¹ Test Video Source</h3>
      <video id="testVideo" controls autoplay playsinline crossorigin="anonymous">
        <source src="https://commondatastorage.googleapis.com/gtv-videos-bucket/sample/BigBuckBunny.mp4"
          type="video/mp4">
        <source src="https://sample-videos.com/zip/10/mp4/SampleVideo_1280x720_1mb.mp4" type="video/mp4">
        Your browser does not support the video tag.
      </video>
      <p><small>ğŸ“ Note: This video may have limited audio. For best testing, use the webcam or speech test buttons
          below.</small></p>
    </div>

    <div class="controls">
      <button onclick="startWebcamVideo()" id="webcamBtn">ğŸ“· Start Webcam Test</button>
      <button onclick="startScreenShare()" id="screenBtn">ğŸ–¥ï¸ Start Screen Share Test</button>
      <button onclick="createSpeechTest()" id="speechBtn">ï¿½ï¸ Start Speech Test</button>
      <button onclick="stopAllStreams()" id="stopBtn">â¹ï¸ Stop All</button>
    </div>

    <div class="console-output" id="consoleOutput">
      <div>Extension console output will appear here...</div>
      <div>Open Chrome DevTools Console for detailed logs.</div>
    </div>
  </div>

  <script>
    let currentStream = null;
    let testAudioContext = null;

    // Override console.log to also display in our output area
    const originalLog = console.log;
    console.log = function (...args) {
      originalLog.apply(console, args);

      const output = document.getElementById('consoleOutput');
      const timestamp = new Date().toLocaleTimeString();
      const message = args.map(arg =>
        typeof arg === 'object' ? JSON.stringify(arg, null, 2) : String(arg)
      ).join(' ');

      output.innerHTML += `<div>[${timestamp}] ${message}</div>`;
      output.scrollTop = output.scrollHeight;
    };

    function updatePageUrl() {
      const platform = document.getElementById('platform').value;
      if (platform !== 'generic') {
        // Simulate being on the platform domain for testing
        console.log(`ğŸŒ Simulating platform: ${platform}`);
        console.log('Extension should detect this as:', platform);
      }
    }

    async function startWebcamVideo() {
      try {
        console.log('ğŸ¥ Requesting webcam access...');
        console.log('âš ï¸ Please grant microphone permission for speech recognition to work!');

        currentStream = await navigator.mediaDevices.getUserMedia({
          video: true,
          audio: true
        });

        const video = document.getElementById('testVideo');
        video.srcObject = currentStream;
        video.muted = false; // Unmute to test audio detection
        video.play(); // Ensure video is playing

        document.getElementById('webcamBtn').disabled = true;
        console.log('âœ… Webcam stream started - Extension should detect this video');
        console.log('ğŸ¤ Now speak into your microphone to test speech recognition!');

        // Add a visual indicator
        const indicator = document.createElement('div');
        indicator.innerHTML = 'ğŸ¤ SPEAK NOW - Microphone is active!';
        indicator.style.cssText = 'position: fixed; top: 10px; right: 10px; background: #4CAF50; color: white; padding: 10px; border-radius: 5px; z-index: 9999; font-weight: bold;';
        document.body.appendChild(indicator);

      } catch (error) {
        console.error('âŒ Error accessing webcam:', error);
        alert('Could not access webcam. Please grant permissions and try again.');
      }
    }

    async function startScreenShare() {
      try {
        console.log('ğŸ–¥ï¸ Requesting screen share access...');
        currentStream = await navigator.mediaDevices.getDisplayMedia({
          video: true,
          audio: true
        });

        const video = document.getElementById('testVideo');
        video.srcObject = currentStream;
        video.muted = false;

        document.getElementById('screenBtn').disabled = true;
        console.log('âœ… Screen share started - Extension should detect this video');

      } catch (error) {
        console.error('âŒ Error accessing screen share:', error);
        alert('Screen share cancelled or not supported.');
      }
    }

    function createSpeechTest() {
      console.log('ğŸ—£ï¸ Creating speech test with microphone...');

      navigator.mediaDevices.getUserMedia({ audio: true })
        .then(stream => {
          console.log('ğŸ¤ Microphone access granted - speak now!');

          // Create a simple test audio context for demonstration
          const audioContext = new (window.AudioContext || window.webkitAudioContext)();
          const source = audioContext.createMediaStreamSource(stream);
          const analyser = audioContext.createAnalyser();
          source.connect(analyser);

          // Show audio levels to indicate the microphone is working
          const dataArray = new Uint8Array(analyser.frequencyBinCount);
          const checkAudio = () => {
            analyser.getByteFrequencyData(dataArray);
            const average = dataArray.reduce((a, b) => a + b) / dataArray.length;
            if (average > 10) {
              console.log('ğŸ”Š Audio detected! Level:', Math.round(average));
            }
          };

          const interval = setInterval(checkAudio, 500);

          // Stop after 30 seconds
          setTimeout(() => {
            clearInterval(interval);
            stream.getTracks().forEach(track => track.stop());
            audioContext.close();
            console.log('â¹ï¸ Speech test ended');
            document.getElementById('speechBtn').disabled = false;
          }, 30000);

          document.getElementById('speechBtn').disabled = true;
          console.log('âœ… Speech test started - speak into your microphone for 30 seconds');
        })
        .catch(error => {
          console.error('âŒ Could not access microphone:', error);
          alert('Please grant microphone permissions for speech testing.');
        });
    }

    function createTestStream() {
      try {
        console.log('ğŸ”Š Creating test audio stream...');

        // Create a simple test tone
        testAudioContext = new (window.AudioContext || window.webkitAudioContext)();
        const oscillator = testAudioContext.createOscillator();
        const gainNode = testAudioContext.createGain();

        oscillator.frequency.setValueAtTime(440, testAudioContext.currentTime); // A note
        gainNode.gain.setValueAtTime(0.1, testAudioContext.currentTime);

        oscillator.connect(gainNode);
        gainNode.connect(testAudioContext.destination);

        oscillator.start();
        oscillator.stop(testAudioContext.currentTime + 2); // Play for 2 seconds

        document.getElementById('streamBtn').disabled = true;
        console.log('âœ… Test audio stream created');

        setTimeout(() => {
          document.getElementById('streamBtn').disabled = false;
        }, 2500);

      } catch (error) {
        console.error('âŒ Error creating test stream:', error);
      }
    }

    function startSynthesizedSpeech() {
      try {
        console.log('ğŸ”Š Starting synthesized speech test...');

        // Create speech synthesis
        if (!window.speechSynthesis) {
          console.error('âŒ Speech synthesis not supported');
          return;
        }

        const testPhrases = [
          "Hello, this is a test of the speech recognition system.",
          "The quick brown fox jumps over the lazy dog.",
          "Meeting video to text extension is working correctly.",
          "Testing speech recognition with synthesized audio.",
          "This demonstrates real-time transcription capabilities."
        ];

        let phraseIndex = 0;

        const speakPhrase = () => {
          if (phraseIndex >= testPhrases.length) {
            console.log('âœ… Synthesized speech test completed');
            document.getElementById('synthBtn').disabled = false;
            return;
          }

          const utterance = new SpeechSynthesisUtterance(testPhrases[phraseIndex]);
          utterance.rate = 0.8; // Slower for better recognition
          utterance.volume = 0.8;
          utterance.pitch = 1.0;

          utterance.onstart = () => {
            console.log(`ğŸ—£ï¸ Speaking: "${testPhrases[phraseIndex]}"`);
          };

          utterance.onend = () => {
            phraseIndex++;
            // Wait 2 seconds between phrases
            setTimeout(speakPhrase, 2000);
          };

          utterance.onerror = (event) => {
            console.error('âŒ Speech synthesis error:', event.error);
            phraseIndex++;
            setTimeout(speakPhrase, 1000);
          };

          window.speechSynthesis.speak(utterance);
        };

        document.getElementById('synthBtn').disabled = true;
        console.log('âœ… Starting synthesized speech - listen for transcription results!');

        // Start speaking phrases
        speakPhrase();

      } catch (error) {
        console.error('âŒ Error with synthesized speech:', error);
      }
    }

    function stopAllStreams() {
      console.log('â¹ï¸ Stopping all streams...');

      // Stop speech synthesis
      if (window.speechSynthesis) {
        window.speechSynthesis.cancel();
      }

      if (currentStream) {
        currentStream.getTracks().forEach(track => track.stop());
        currentStream = null;
      }

      if (testAudioContext) {
        testAudioContext.close();
        testAudioContext = null;
      }

      const video = document.getElementById('testVideo');
      video.srcObject = null;
      video.load(); // Reset to default source

      // Re-enable buttons
      document.getElementById('webcamBtn').disabled = false;
      document.getElementById('screenBtn').disabled = false;
      document.getElementById('synthBtn').disabled = false;

      // Remove any indicators
      const indicators = document.querySelectorAll('[style*="position: fixed"]');
      indicators.forEach(indicator => indicator.remove());

      console.log('âœ… All streams stopped');
    }

    // Simulate meeting platform indicators
    function addMeetingIndicators() {
      // Add some meeting-like elements to help the extension detect the "meeting" context
      const indicator = document.createElement('div');
      indicator.setAttribute('data-meeting-video', 'true');
      indicator.setAttribute('data-allocation-index', '0');
      indicator.style.display = 'none';
      document.body.appendChild(indicator);

      console.log('ğŸ·ï¸ Added meeting platform indicators');
    }

    // Initialize
    document.addEventListener('DOMContentLoaded', () => {
      console.log('ğŸ¬ Test page loaded');
      console.log('Extension should initialize and start detecting videos...');
      addMeetingIndicators();

      // Add some realistic video attributes
      const video = document.getElementById('testVideo');
      video.setAttribute('data-meeting-video', 'true');
      video.setAttribute('autoplay', 'true');
    });

    // Monitor extension activity
    setInterval(() => {
      // Check if extension is working
      if (window.meetingVideoToTextExtension) {
        console.log('ğŸ“Š Extension detected and active');
      }
    }, 10000);
  </script>
</body>

</html>